 uv pip install thrml jax[cuda12]
python csv_compress.py your_data_quantized.npy csv_compressor.thrml.py
# Ultra-efficient CSV compression via THRML + Extropic-style block Gibbs
# Runs today on GPU, tomorrow on Extropic thermodynamic chips

import jax
import jax.numpy as jnp
import jax.random as jr
from jax import jit, vmap
from functools import partial

from thrml import (
    CategoricalNode, SpinNode, Block,
    SamplingSchedule, sample_states,
    hinton_init
)
from thrml.models import FactorGraphEBM, HeteroSamplingProgram
from thrml.factors import TableFactor, RNNTransitionFactor, GNNMessageFactor

# ========================================================
# 1. Hyperparameters & Vocabulary Setup (learned per dataset)
# ========================================================

N_ROWS = 8192              # Number of rows in chunk (power-of-2 friendly)
N_COLS = 64                # Max columns (padded)
MAX_VOCAB_PER_COL = 128    # Quantized to 7 bits max per field
LATENT_DIM = 8             # Bits per latent symbol → 256-ary codes

# Learned column vocabularies (in practice: fit via discrete autoencoder)
col_vocabs = [MAX_VOCAB_PER_COL] * N_COLS
col_types = ['cat'] * N_COLS  # All treated as categorical after quantization

# ========================================================
# 2. Define Nodes
# ========================================================

# Observed quantized CSV tokens (clamped during inference)
obs_nodes = [
    [CategoricalNode(size=col_vocabs[c]) for c in range(N_COLS)]
    for r in range(N_ROWS)
]
obs_nodes = sum(obs_nodes, [])  # Flatten to list

# Latent compression codes (one per row)
latent_nodes = [CategoricalNode(size=2**LATENT_DIM) for _ in range(N_ROWS)]

# Optional: hierarchical row-type spins (document structure)
row_type_nodes = [SpinNode() for _ in range(N_ROWS)]  # e.g., header, data, footer

all_nodes = obs_nodes + latent_nodes + row_type_nodes

# ========================================================
# 3. Define Factors (Energy Terms)
# ========================================================

factors = []

# 3a) Reconstruction: each row's latent code must predict all its tokens
for r in range(N_ROWS):
    row_obs = obs_nodes[r*N_COLS : (r+1)*N_COLS]
    z = latent_nodes[r]
    # Learned lookup table: P(obs_row | z)
    table = TableFactor(
        variables=row_obs + [z],
        table=jax.random.normal(jr.key(r), (*(col_vocabs), 2**LATENT_DIM)),
        name=f"recon_r{r}"
    )
    factors.append(table)

# 3b) RNN-style sequential prior over latents → temporal compression
for r in range(N_ROWS-1):
    trans = RNNTransitionFactor(
        # Custom GRU-like discrete transition
        variables=[latent_nodes[r], latent_nodes[r+1]],
        hidden_size=256,
        n_layers=2,
        init_scale=0.02
    )
    factors.append(trans)

# 3c) GNN over column similarity graph (learned column affinities)
column_graph = jnp.ones((N_COLS, N_COLS)) - jnp.eye(N_COLS)  # placeholder
for r in range(N_ROWS):
    gnn_factor = GNNMessageFactor(
        row_latent=latent_nodes[r],
        column_tokens=obs_nodes[r*N_COLS:(r+1)*N_COLS],
        adjacency=column_graph,
        n_message_passes=2,
        msg_dim=64
    )
    factors.append(gnn_factor)

# 3d) Optional: encourage structured row types
for r in range(N_ROWS-1):
    factors.append(
        TableFactor([row_type_nodes[r], row_type_nodes[r+1]],
                    table=jnp.array([[1.0, -1.0], [-1.0, 1.0]]),  # prefer same type
                    name=f"type_trans{r}")
    )

# ========================================================
# 4. Build the Full EBM
# ========================================================

model = FactorGraphEBM(nodes=all_nodes, factors=factors)

# ========================================================
# 5. Block Gibbs Scheduling (Extropic-optimized)
# ========================================================

# Fast parallel blocks
latent_block = Block(latent_nodes)                    # All latents together
row_type_block = Block(row_type_nodes)
column_blocks = [Block(obs_nodes[r*N_COLS:(r+1)*N_COLS]) for r in range(N_ROWS)]

free_blocks = [latent_block, row_type_block] + column_blocks

# During compression: observe data, infer latents
# During decompression: clamp latents, sample observations
program = HeteroSamplingProgram(model, free_blocks=free_blocks, clamped_blocks=[])

# ========================================================
# 6. Compression Function (JIT-compiled)
# ========================================================

@partial(jit, static_argnums=(2,))
def compress_csv(quantized_csv_batch: jnp.ndarray,            # shape [N_ROWS, N_COLS]
                 key,
                 n_samples: int = 2000) -> jnp.ndarray:       # returns latent codes [N_ROWS]
    key_init, key_samp = jr.split(key)

    # Clamp observed tokens
    clamp_dict = {}
    for r in range(N_ROWS):
        for c in range(N_COLS):
            clamp_dict[obs_nodes[r*N_COLS + c]] = quantized_csv[r, c]

    init_state = hinton_init(key_init, model, free_blocks, clamp_dict)

    schedule = SamplingSchedule(
        n_warmup=500,
        n_samples=n_samples,
        steps_per_sample=1
    )

    samples = sample_states(key_samp, program, schedule, init_state,
                           clamped_values=[clamp_dict],
                           collect_blocks=[latent_block])

    # Return MAP latent codes
    latent_samples = samples[latent_block][0]  # [n_samples, N_ROWS]
    counts = jax.nn.one_hot(latent_samples, 2**LATENT_DIM).sum(axis=0)
    map_codes = jnp.argmax(counts, axis=-1)
    return map_codes.astype(jnp.uint8)  # 8 bits per row → ~90–98% compression


# ========================================================
# 7. Decompression (reverse)
# ========================================================

@jit
def decompress(latent_codes: jnp.ndarray, key) -> jnp.ndarray:
    clamp_dict = {latent_nodes[r]: int(latent_codes[r]) for r in range(N_ROWS)}
    init_state = hinton_init(key, model, free_blocks, clamp_dict)

    schedule = SamplingSchedule(n_warmup=100, n_samples=100, steps_per_sample=2)
    samples = sample_states(key, program, schedule, init_state,
                           clamped_values=[clamp_dict],
                           collect_blocks=[Block(obs_nodes)])

    # Vote over columns
    obs_samples = samples[Block(obs_nodes)][0]
    reconstructed = jnp.argmax(jax.nn.one_hot(obs_samples, MAX_VOCAB_PER_COL).sum(0),
                               axis=-1)
    return reconstructed.reshape(N_ROWS, N_COLS)